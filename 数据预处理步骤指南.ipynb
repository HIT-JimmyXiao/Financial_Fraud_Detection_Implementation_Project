{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# è´¢åŠ¡èˆå¼Šè¯†åˆ«å®éªŒ1 - æ•°æ®é¢„å¤„ç†æ­¥éª¤æŒ‡å—\n",
        "\n",
        "æœ¬NotebookåŸºäº `preprocess_data_balanced.py` è„šæœ¬ï¼Œå°†æ•°æ®é¢„å¤„ç†æµç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œæ–¹ä¾¿é€æ­¥æ‰§è¡Œå’Œè°ƒè¯•ã€‚\n",
        "\n",
        "## ğŸ“‹ æµç¨‹æ¦‚è§ˆ\n",
        "\n",
        "1. **Step 0**: ç¯å¢ƒå‡†å¤‡ä¸å¯¼å…¥\n",
        "2. **Step 1**: æ•°æ®ç†è§£ä¸æ¢ç´¢\n",
        "3. **Step 2**: è¯»å–å’Œè§„èŒƒåŒ–å•ä¸ªè¡¨\n",
        "4. **Step 3**: æ•°æ®é›†æˆï¼ˆæ¨ªå‘å¹¶è¡¨ï¼‰\n",
        "5. **Step 4**: ç”Ÿæˆè¿è§„æ ‡ç­¾\n",
        "6. **Step 5**: æ•°æ®æ¸…æ´—\n",
        "7. **Step 6**: ç”ŸæˆSTæ ‡ç­¾\n",
        "8. **Step 7**: å‡†å¤‡æœ€ç»ˆè¾“å‡º\n",
        "9. **Step 8**: è´¨é‡æ ¡éªŒ\n",
        "10. **Step 9**: å¯¼å‡ºæ–‡ä»¶\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "- æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªæ­¥éª¤çš„ä»£ç å•å…ƒæ ¼\n",
        "- æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ•°æ®è´¨é‡\n",
        "- å¯ä»¥éšæ—¶æŸ¥çœ‹ä¸­é—´ç»“æœ\n",
        "- é‡åˆ°é—®é¢˜å¯ä»¥å•ç‹¬è°ƒè¯•æŸä¸ªæ­¥éª¤\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: ç¯å¢ƒå‡†å¤‡ä¸å¯¼å…¥\n",
        "\n",
        "å¯¼å…¥å¿…è¦çš„åº“ï¼Œè®¾ç½®å·¥ä½œç›®å½•å’Œå‚æ•°ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®å‚æ•°\n",
        "DATA_DIR = 'Dataset'\n",
        "OUTPUT_DIR = 'Insight_output'\n",
        "GROUP_ID = '1'\n",
        "\n",
        "# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# æ•°æ®æ–‡ä»¶è·¯å¾„é…ç½®\n",
        "data_files = {\n",
        "    'solvency': os.path.join(DATA_DIR, 'å¿å€ºèƒ½åŠ›', 'FI_T1.xlsx'),\n",
        "    'disclosure': os.path.join(DATA_DIR, 'æŠ«éœ²è´¢åŠ¡æŒ‡æ ‡', 'FI_T2.xlsx'),\n",
        "    'operation': os.path.join(DATA_DIR, 'ç»è¥èƒ½åŠ›', 'FI_T4.xlsx'),\n",
        "    'profit': os.path.join(DATA_DIR, 'ç›ˆåˆ©èƒ½åŠ›', 'FI_T5.xlsx'),\n",
        "    'risk': os.path.join(DATA_DIR, 'é£é™©æ°´å¹³', 'FI_T7.xlsx'),\n",
        "    'growth': os.path.join(DATA_DIR, 'å‘å±•èƒ½åŠ›', 'FI_T8.xlsx'),\n",
        "    'pershare': os.path.join(DATA_DIR, 'æ¯è‚¡æŒ‡æ ‡', 'FI_T9.xlsx'),\n",
        "    'dividend': os.path.join(DATA_DIR, 'è‚¡åˆ©åˆ†é…', 'FI_T11.xlsx'),\n",
        "    'violation': os.path.join(DATA_DIR, 'è¿è§„ä¿¡æ¯æ€»è¡¨', 'STK_Violation_Main.xlsx')\n",
        "}\n",
        "\n",
        "# Typrep ä¼˜å…ˆçº§ï¼šK(åˆå¹¶) > C(åˆå¹¶è°ƒæ•´) > A(å¹´æŠ¥) > B(åŠå¹´æŠ¥) > å…¶ä»–\n",
        "typrep_priority = {'K': 1, 'C': 2, 'A': 3, 'B': 4, 'S': 5, 'H': 6, 'F': 7, 'E': 8, 'N': 9}\n",
        "\n",
        "# æœ€ç»ˆåˆ—é¡ºåº\n",
        "column_order = [\n",
        "    'Stkcd', 'Accper', 'Typrep', 'Indcd', 'isviolation', 'isST',\n",
        "    'F040101B', 'F040202B', 'F040203B', 'F040205C', 'F040401B', 'F040503B', 'F040505C',\n",
        "    'F040803B', 'F040805C', 'F041203B', 'F041205C', 'F041301B', 'F041403B', 'F041405C',\n",
        "    'F041703B', 'F041705C', 'F041803B', 'F041805C',\n",
        "    'F050104C', 'F050204C', 'F053201B', 'F053301C', 'F052401B', 'F053202B',\n",
        "    'F080102A', 'F081002B', 'F082601B', 'F080603A',\n",
        "    'F070101B', 'F070201B', 'F070301B',\n",
        "    'F090102B', 'F020108',\n",
        "    'F110101B', 'F110301B', 'F110801B'\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"è´¢åŠ¡èˆå¼Šè¯†åˆ«å®éªŒ1 - æ•°æ®é¢„å¤„ç†ï¼ˆå¹³è¡¡ç‰ˆï¼‰\")\n",
        "print(\"ç­–ç•¥ï¼šä¸‰é”® (Stkcd, Year, Typrep) + æ™ºèƒ½å»é‡é¿å…æ•°æ®çˆ†ç‚¸\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nå¼€å§‹æ—¶é—´: {datetime.now()}\")\n",
        "print(f\"æ•°æ®ç›®å½•: {DATA_DIR}\")\n",
        "print(f\"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
        "print(f\"ç»„å·: {GROUP_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: æ•°æ®ç†è§£ä¸æ¢ç´¢\n",
        "\n",
        "æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ŒæŸ¥çœ‹æ•°æ®çš„åŸºæœ¬ä¿¡æ¯ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "print(\"æ£€æŸ¥æ•°æ®æ–‡ä»¶...\")\n",
        "for name, filepath in data_files.items():\n",
        "    exists = os.path.exists(filepath)\n",
        "    status = \"âœ“\" if exists else \"âœ—\"\n",
        "    print(f\"{status} {name:15s}: {os.path.basename(filepath)}\")\n",
        "\n",
        "# è¯»å–ä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶æŸ¥çœ‹ç»“æ„ï¼ˆç»è¥èƒ½åŠ›è¡¨ï¼‰\n",
        "if os.path.exists(data_files['operation']):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ç¤ºä¾‹ï¼šç»è¥èƒ½åŠ›è¡¨ (FI_T4.xlsx) å‰5è¡Œ\")\n",
        "    print(\"=\" * 80)\n",
        "    df_sample = pd.read_excel(data_files['operation'], nrows=5)\n",
        "    print(f\"åˆ—æ•°: {len(df_sample.columns)}\")\n",
        "    print(f\"åˆ—å: {list(df_sample.columns)}\")\n",
        "    print(\"\\næ•°æ®é¢„è§ˆ:\")\n",
        "    print(df_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: è¾…åŠ©å‡½æ•°å®šä¹‰\n",
        "\n",
        "å®šä¹‰æ•°æ®é¢„å¤„ç†æ‰€éœ€çš„è¾…åŠ©å‡½æ•°ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_excel_safe(filepath):\n",
        "    \"\"\"å®‰å…¨è¯»å–Excelæ–‡ä»¶\"\"\"\n",
        "    try:\n",
        "        print(f\"æ­£åœ¨è¯»å–: {os.path.basename(filepath)}\")\n",
        "        df = pd.read_excel(filepath)\n",
        "        print(f\"  æˆåŠŸè¯»å– {len(df)} è¡Œ\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"  è¯»å–å¤±è´¥: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def standardize_stock_code(code):\n",
        "    \"\"\"æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½å­—ç¬¦ä¸²\"\"\"\n",
        "    if pd.isna(code):\n",
        "        return None\n",
        "    try:\n",
        "        return str(int(code)).zfill(6)\n",
        "    except:\n",
        "        return str(code).zfill(6)\n",
        "\n",
        "def extract_year(date_str):\n",
        "    \"\"\"ä»æ—¥æœŸå­—ç¬¦ä¸²æå–å¹´ä»½\"\"\"\n",
        "    if pd.isna(date_str):\n",
        "        return None\n",
        "    try:\n",
        "        if isinstance(date_str, str):\n",
        "            return int(date_str.split('-')[0])\n",
        "        elif isinstance(date_str, (pd.Timestamp, datetime)):\n",
        "            return date_str.year\n",
        "        else:\n",
        "            return int(date_str)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "print(\"è¾…åŠ©å‡½æ•°å®šä¹‰å®Œæˆ âœ“\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: è¯»å–å’Œè§„èŒƒåŒ–å•ä¸ªè¡¨\n",
        "\n",
        "é€ä¸ªè¯»å–è´¢åŠ¡è¡¨ï¼Œè¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š\n",
        "- å¤„ç†ä¸­æ–‡åˆ—åï¼ˆå¿å€ºèƒ½åŠ›è¡¨ï¼‰\n",
        "- æ ‡å‡†åŒ–é”®å­—æ®µï¼ˆStkcd, Accper, Typrepï¼‰\n",
        "- æ™ºèƒ½å»é‡ï¼ˆæŒ‰Typrepä¼˜å…ˆçº§ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_prepare_table(name, filepath):\n",
        "    \"\"\"è¯»å–ã€è§„èŒƒåŒ–å¹¶æ™ºèƒ½å»é‡å•ä¸ªä¸»é¢˜è¡¨\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"âš ï¸ {name} æ–‡ä»¶ä¸å­˜åœ¨\")\n",
        "        return None\n",
        "    \n",
        "    df = read_excel_safe(filepath)\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    # å¤„ç†å¿å€ºèƒ½åŠ›è¡¨çš„ä¸­æ–‡åˆ—å\n",
        "    if name == 'solvency':\n",
        "        df.columns = [col.strip().strip(\"'\") for col in df.columns]\n",
        "        column_mapping = {}\n",
        "        for col in df.columns:\n",
        "            if 'è‚¡ç¥¨ä»£ç ' in col:\n",
        "                column_mapping[col] = 'Stkcd'\n",
        "            elif 'æˆªæ­¢æ—¥æœŸ' in col:\n",
        "                column_mapping[col] = 'Accper'\n",
        "            elif 'æŠ¥è¡¨ç±»å‹ç¼–ç ' in col:\n",
        "                column_mapping[col] = 'Typrep'\n",
        "            elif 'è¡Œä¸šä»£ç ' in col or col == 'Indcd':\n",
        "                column_mapping[col] = 'Indcd'\n",
        "            elif 'æµåŠ¨æ¯”ç‡' in col:\n",
        "                column_mapping[col] = 'F010101A'\n",
        "            elif 'é€ŸåŠ¨æ¯”ç‡' in col:\n",
        "                column_mapping[col] = 'F010201A'\n",
        "            elif 'åˆ©æ¯ä¿éšœå€æ•°B' in col:\n",
        "                column_mapping[col] = 'F010702B'\n",
        "            elif 'ç°é‡‘æµé‡' in col and 'æµåŠ¨è´Ÿå€º' in col:\n",
        "                column_mapping[col] = 'F010801B'\n",
        "            elif 'èµ„äº§è´Ÿå€ºç‡' in col:\n",
        "                column_mapping[col] = 'F011201A'\n",
        "        df = df.rename(columns=column_mapping)\n",
        "    \n",
        "    # æ ‡å‡†åŒ–é”®å­—æ®µ\n",
        "    if 'Stkcd' in df.columns:\n",
        "        df['Stkcd_std'] = df['Stkcd'].apply(standardize_stock_code)\n",
        "    if 'Accper' in df.columns:\n",
        "        df['Year'] = df['Accper'].apply(extract_year)\n",
        "    \n",
        "    # æ¸…æ´—ï¼šç§»é™¤æ— æ•ˆè®°å½•\n",
        "    before = len(df)\n",
        "    df = df[df['Stkcd_std'].notna() & df['Year'].notna()].copy()\n",
        "    after = len(df)\n",
        "    if before > after:\n",
        "        print(f\"  æ¸…æ´—æ— æ•ˆé”®: -{before - after} è¡Œ\")\n",
        "    \n",
        "    # å…³é”®ä¼˜åŒ–ï¼šæŒ‰ Typrep ä¼˜å…ˆçº§ï¼Œæ¯ä¸ª (Stkcd, Year, Typrep) åªä¿ç•™ä¸€æ¡\n",
        "    if 'Typrep' in df.columns:\n",
        "        df = df[df['Typrep'].notna()].copy()\n",
        "        df['typrep_priority'] = df['Typrep'].map(typrep_priority).fillna(99)\n",
        "        \n",
        "        # æŒ‰ä¼˜å…ˆçº§æ’åºåå»é‡\n",
        "        df = df.sort_values(['Stkcd_std', 'Year', 'Typrep', 'typrep_priority'])\n",
        "        df = df.drop_duplicates(subset=['Stkcd_std', 'Year', 'Typrep'], keep='first')\n",
        "        df = df.drop(columns=['typrep_priority'])\n",
        "        \n",
        "        print(f\"  å»é‡å: {len(df)} è¡Œ, æŠ¥è¡¨ç±»å‹: {df['Typrep'].value_counts().to_dict()}\")\n",
        "    else:\n",
        "        # æ—  Typrep çš„è¡¨ï¼ˆå¦‚æŠ«éœ²è´¢åŠ¡ï¼‰ï¼ŒæŒ‰ (Stkcd, Year) å»é‡\n",
        "        df = df.drop_duplicates(subset=['Stkcd_std', 'Year'], keep='first')\n",
        "        print(f\"  å»é‡å: {len(df)} è¡Œ\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# æµ‹è¯•ï¼šè¯»å–ä¸€ä¸ªè¡¨\n",
        "print(\"=\" * 80)\n",
        "print(\"æµ‹è¯•ï¼šè¯»å–ç»è¥èƒ½åŠ›è¡¨\")\n",
        "print(\"=\" * 80)\n",
        "test_df = load_and_prepare_table('operation', data_files['operation'])\n",
        "if test_df is not None:\n",
        "    print(f\"\\nâœ“ æˆåŠŸè¯»å–å¹¶è§„èŒƒåŒ–\")\n",
        "    print(f\"æ•°æ®å½¢çŠ¶: {test_df.shape}\")\n",
        "    print(f\"åˆ—å: {list(test_df.columns)[:10]}...\")  # æ˜¾ç¤ºå‰10åˆ—\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 4: æ•°æ®é›†æˆ - æ¨ªå‘å¹¶è¡¨ï¼ˆä¸‰é”®ç­–ç•¥ï¼‰\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# åŠ è½½æ‰€æœ‰ä¸»é¢˜è¡¨\n",
        "dfs = {}\n",
        "for name, filepath in data_files.items():\n",
        "    if name != 'violation':\n",
        "        df = load_and_prepare_table(name, filepath)\n",
        "        if df is not None:\n",
        "            dfs[name] = df\n",
        "\n",
        "print(f\"\\næˆåŠŸåŠ è½½ {len(dfs)} ä¸ªè´¢åŠ¡è¡¨\")\n",
        "\n",
        "# ç¡®å®šä¸»é”®é›†åˆï¼šä»ç»è¥èƒ½åŠ›è¡¨å¼€å§‹\n",
        "if 'operation' not in dfs:\n",
        "    print(\"âŒ ç¼ºå°‘ç»è¥èƒ½åŠ›è¡¨ï¼\")\n",
        "else:\n",
        "    base_df = dfs['operation'].copy()\n",
        "    merge_keys = ['Stkcd_std', 'Year', 'Typrep']\n",
        "    \n",
        "    # é€‰æ‹©åŸºå‡†è¡¨çš„æŒ‡æ ‡åˆ—å’Œå…¶ä»–é‡è¦å­—æ®µ\n",
        "    indicator_cols = [c for c in base_df.columns if c.startswith('F')]\n",
        "    other_cols = [c for c in base_df.columns if c in ['Indcd']]\n",
        "    keep_cols = merge_keys + other_cols + indicator_cols\n",
        "    keep_cols = [c for c in keep_cols if c in base_df.columns]\n",
        "    base_df = base_df[keep_cols].copy()\n",
        "    \n",
        "    print(f\"\\nåŸºå‡†è¡¨ï¼ˆç»è¥èƒ½åŠ›ï¼‰: {len(base_df)} è¡Œ\")\n",
        "    \n",
        "    # ä¾æ¬¡åˆå¹¶å…¶ä»–è¡¨ï¼ˆä½¿ç”¨ outer join ä¿ç•™æ‰€æœ‰æ•°æ®ï¼‰\n",
        "    merge_order = ['profit', 'growth', 'solvency', 'risk', 'pershare', 'dividend']\n",
        "    \n",
        "    for name in merge_order:\n",
        "        if name not in dfs:\n",
        "            print(f\"âš ï¸ {name} è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡\")\n",
        "            continue\n",
        "        \n",
        "        df = dfs[name].copy()\n",
        "        \n",
        "        # é€‰æ‹©æŒ‡æ ‡åˆ—å’Œå…¶ä»–é‡è¦å­—æ®µ\n",
        "        indicator_cols = [c for c in df.columns if c.startswith('F')]\n",
        "        other_cols = [c for c in df.columns if c in ['Indcd']]\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦æœ‰ Typrep\n",
        "        has_typrep = 'Typrep' in df.columns\n",
        "        \n",
        "        if has_typrep:\n",
        "            keep_cols_df = merge_keys + other_cols + indicator_cols\n",
        "            keys_to_use = merge_keys\n",
        "        else:\n",
        "            # æ—  Typrep çš„è¡¨ï¼Œåªä½¿ç”¨ (Stkcd, Year)\n",
        "            keep_cols_df = ['Stkcd_std', 'Year'] + other_cols + indicator_cols\n",
        "            keys_to_use = ['Stkcd_std', 'Year']\n",
        "        \n",
        "        keep_cols_df = [c for c in keep_cols_df if c in df.columns]\n",
        "        df_subset = df[keep_cols_df].copy()\n",
        "        \n",
        "        print(f\"\\nåˆå¹¶ {name} è¡¨...\")\n",
        "        print(f\"  åˆå¹¶å‰: {len(base_df)} è¡Œ\")\n",
        "        print(f\"  å¾…åˆå¹¶: {len(df_subset)} è¡Œ (ä½¿ç”¨é”®: {keys_to_use})\")\n",
        "        \n",
        "        # ä½¿ç”¨ outer join ä¿ç•™æ‰€æœ‰è®°å½•\n",
        "        base_df = base_df.merge(df_subset, on=keys_to_use, how='outer', suffixes=('', '_dup'))\n",
        "        \n",
        "        # åˆ é™¤é‡å¤åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "        dup_cols = [c for c in base_df.columns if c.endswith('_dup')]\n",
        "        if dup_cols:\n",
        "            base_df = base_df.drop(columns=dup_cols)\n",
        "        \n",
        "        print(f\"  åˆå¹¶å: {len(base_df)} è¡Œ\")\n",
        "    \n",
        "    print(f\"\\nâœ“ æœ€ç»ˆé›†æˆæ•°æ®: {len(base_df)} è¡Œ, {len(base_df.columns)} åˆ—\")\n",
        "    merged_df = base_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 5: ç”Ÿæˆè¿è§„æ ‡ç­¾\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "filepath = data_files['violation']\n",
        "if not os.path.exists(filepath):\n",
        "    print(\"âš ï¸ è¿è§„ä¿¡æ¯è¡¨ä¸å­˜åœ¨\")\n",
        "    merged_df['isviolation'] = 0\n",
        "else:\n",
        "    violation_df = read_excel_safe(filepath)\n",
        "    if violation_df is None:\n",
        "        merged_df['isviolation'] = 0\n",
        "    else:\n",
        "        # æ ‡å‡†åŒ–è¿è§„è¡¨çš„é”®\n",
        "        violation_df['Stkcd_std'] = violation_df['Symbol'].apply(standardize_stock_code)\n",
        "        violation_df['Year'] = pd.to_numeric(violation_df['ViolationYear'], errors='coerce')\n",
        "        violation_df = violation_df[violation_df['Year'].notna()].copy()\n",
        "        violation_df['Year'] = violation_df['Year'].astype(int)\n",
        "        \n",
        "        # ç¡®ä¿ä¸»è¡¨çš„ Year æ˜¯ int\n",
        "        merged_df['Year'] = merged_df['Year'].astype(int)\n",
        "        \n",
        "        # ç­›é€‰å®é™…è¿è§„è®°å½•ï¼ˆIsViolated='Y'ï¼‰\n",
        "        violation_yes = violation_df[violation_df['IsViolated'] == 'Y'].copy()\n",
        "        print(f\"å®é™…è¿è§„è®°å½•æ•°: {len(violation_yes)}\")\n",
        "        \n",
        "        # æŒ‰ (Stkcd_std, Year) èšåˆï¼ˆè¿è§„æ˜¯é’ˆå¯¹å…¬å¸å¹´åº¦ï¼Œä¸åˆ†æŠ¥è¡¨ç±»å‹ï¼‰\n",
        "        violation_set = violation_yes.groupby(['Stkcd_std', 'Year']).size().reset_index(name='violation_count')\n",
        "        violation_set['isviolation'] = 1\n",
        "        \n",
        "        print(f\"è¿è§„çš„ (å…¬å¸, å¹´ä»½) æ•°: {len(violation_set)}\")\n",
        "        \n",
        "        # åˆå¹¶åˆ°ä¸»è¡¨\n",
        "        merged_df = merged_df.merge(\n",
        "            violation_set[['Stkcd_std', 'Year', 'isviolation']], \n",
        "            on=['Stkcd_std', 'Year'], \n",
        "            how='left'\n",
        "        )\n",
        "        merged_df['isviolation'] = merged_df['isviolation'].fillna(0).astype(int)\n",
        "        \n",
        "        print(f\"âœ“ è¿è§„æ ·æœ¬: {merged_df['isviolation'].sum()} ({merged_df['isviolation'].mean():.2%})\")\n",
        "\n",
        "# æ˜¾ç¤ºè¿è§„æ ·æœ¬ç»Ÿè®¡\n",
        "print(\"\\nè¿è§„æ ·æœ¬ç»Ÿè®¡:\")\n",
        "print(merged_df['isviolation'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: æ•°æ®æ¸…æ´—\n",
        "\n",
        "å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 6: æ•°æ®æ¸…æ´—\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ç»Ÿä¸€ç¼ºå¤±å€¼\n",
        "cleaned_df = merged_df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# ç»Ÿè®¡ç¼ºå¤±å€¼\n",
        "missing_pct = (cleaned_df.isnull().sum() / len(cleaned_df) * 100).round(2)\n",
        "high_missing = missing_pct[missing_pct > 10].sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆç¼ºå¤±ç‡>10%çš„åˆ—ï¼‰:\")\n",
        "for col, pct in high_missing.head(10).items():\n",
        "    print(f\"  {col}: {pct}%\")\n",
        "\n",
        "# ç¡®ä¿æŒ‡æ ‡åˆ—ä¸ºæ•°å€¼ç±»å‹\n",
        "numeric_cols = [c for c in cleaned_df.columns if c.startswith('F')]\n",
        "for col in numeric_cols:\n",
        "    if col in cleaned_df.columns:\n",
        "        cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')\n",
        "\n",
        "print(f\"\\nâœ“ å·²ç¡®ä¿ {len(numeric_cols)} ä¸ªæŒ‡æ ‡åˆ—ä¸ºæ•°å€¼ç±»å‹\")\n",
        "print(f\"æ•°æ®å½¢çŠ¶: {cleaned_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: ç”ŸæˆSTæ ‡ç­¾\n",
        "\n",
        "åŸºäºè´¢åŠ¡å¼‚å¸¸æŒ‡æ ‡ç”Ÿæˆ `isST` æ ‡ç­¾ï¼ˆSTè­¦ç¤ºæ ‡è®°ï¼‰ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nç”ŸæˆSTæ ‡ç­¾...\")\n",
        "\n",
        "# åˆå§‹åŒ–isSTä¸º0\n",
        "cleaned_df['isST'] = 0\n",
        "\n",
        "# è§„åˆ™1: èµ„ä¸æŠµå€ºï¼ˆèµ„äº§è´Ÿå€ºç‡>100%ï¼Œå³å‡€èµ„äº§ä¸ºè´Ÿï¼‰\n",
        "if 'F011201A' in cleaned_df.columns:\n",
        "    condition_insolvency = cleaned_df['F011201A'] > 1.0  # èµ„äº§è´Ÿå€ºç‡>100%\n",
        "    cleaned_df.loc[condition_insolvency, 'isST'] = 1\n",
        "    print(f\"  è§„åˆ™1ï¼ˆèµ„ä¸æŠµå€ºï¼‰: {condition_insolvency.sum()} æ¡\")\n",
        "\n",
        "# è§„åˆ™2: å‡€åˆ©æ¶¦ä¸ºè´Ÿï¼ˆROAä¸ºè´Ÿï¼‰\n",
        "if 'F050204C' in cleaned_df.columns:\n",
        "    condition_negative_profit = cleaned_df['F050204C'] < 0  # ROA<0è¡¨ç¤ºå‡€åˆ©æ¶¦ä¸ºè´Ÿ\n",
        "else:\n",
        "    condition_negative_profit = False\n",
        "\n",
        "# è§„åˆ™3: æœ‰è¿è§„è®°å½•\n",
        "condition_violation = cleaned_df['isviolation'] == 1\n",
        "\n",
        "# ç»¼åˆåˆ¤æ–­ï¼šæ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å³æ ‡è®°ä¸ºST\n",
        "# 1. èµ„ä¸æŠµå€ºï¼ˆå‡€èµ„äº§ä¸ºè´Ÿï¼‰\n",
        "# 2. å‡€åˆ©æ¶¦ä¸ºè´Ÿ ä¸” æœ‰è¿è§„è®°å½•\n",
        "cleaned_df.loc[condition_negative_profit & condition_violation, 'isST'] = 1\n",
        "print(f\"  è§„åˆ™2ï¼ˆå‡€åˆ©æ¶¦ä¸ºè´Ÿ+è¿è§„ï¼‰: {(condition_negative_profit & condition_violation).sum()} æ¡\")\n",
        "\n",
        "# ç‰¹åˆ«å¤„ç†ï¼šè¿ç»­è¿è§„çš„å…¬å¸æ›´å¯èƒ½è¢«ST\n",
        "# æŒ‰å…¬å¸åˆ†ç»„ï¼Œè®¡ç®—ç´¯è®¡è¿è§„æ¬¡æ•°\n",
        "if 'Stkcd_std' in cleaned_df.columns and 'Year' in cleaned_df.columns:\n",
        "    cleaned_df = cleaned_df.sort_values(['Stkcd_std', 'Year'])\n",
        "    cleaned_df['cumulative_violations'] = cleaned_df.groupby('Stkcd_std')['isviolation'].cumsum()\n",
        "    # ç´¯è®¡è¿è§„>=2æ¬¡çš„æ ‡è®°ä¸ºST\n",
        "    condition_cumulative = cleaned_df['cumulative_violations'] >= 2\n",
        "    cleaned_df.loc[condition_cumulative, 'isST'] = 1\n",
        "    print(f\"  è§„åˆ™3ï¼ˆç´¯è®¡è¿è§„>=2æ¬¡ï¼‰: {condition_cumulative.sum()} æ¡\")\n",
        "    cleaned_df = cleaned_df.drop(columns=['cumulative_violations'])\n",
        "\n",
        "st_count = cleaned_df['isST'].sum()\n",
        "st_rate = cleaned_df['isST'].mean()\n",
        "print(f\"\\nâœ“ STæ ·æœ¬: {st_count} ({st_rate:.2%})\")\n",
        "print(f\"STåˆ¤æ–­ä¾æ®:\")\n",
        "print(f\"  - èµ„ä¸æŠµå€ºï¼ˆèµ„äº§è´Ÿå€ºç‡>100%ï¼‰\")\n",
        "print(f\"  - å‡€åˆ©æ¶¦ä¸ºè´Ÿ + æœ‰è¿è§„è®°å½•\")\n",
        "print(f\"  - ç´¯è®¡è¿è§„>=2æ¬¡\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "douxiej"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 8: å‡†å¤‡æœ€ç»ˆè¾“å‡º\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# æ¢å¤åŸå§‹åˆ—å\n",
        "cleaned_df['Stkcd'] = cleaned_df['Stkcd_std'].apply(lambda x: int(x) if pd.notna(x) else np.nan)\n",
        "cleaned_df['Accper'] = cleaned_df['Year']\n",
        "\n",
        "# ç¡®ä¿ Typrep å­˜åœ¨\n",
        "if 'Typrep' not in cleaned_df.columns:\n",
        "    cleaned_df['Typrep'] = ''\n",
        "\n",
        "# ç¡®ä¿ Indcd å­˜åœ¨\n",
        "if 'Indcd' not in cleaned_df.columns:\n",
        "    cleaned_df['Indcd'] = ''\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰å¿…éœ€åˆ—å­˜åœ¨\n",
        "for col in column_order:\n",
        "    if col not in cleaned_df.columns:\n",
        "        if col in ['isviolation', 'isST']:\n",
        "            cleaned_df[col] = 0\n",
        "        elif col not in ['Stkcd', 'Accper', 'Typrep', 'Indcd']:\n",
        "            cleaned_df[col] = np.nan\n",
        "\n",
        "# é€‰æ‹©å¹¶æ’åºåˆ—\n",
        "output_df = cleaned_df[column_order].copy()\n",
        "\n",
        "# åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ\n",
        "before = len(output_df)\n",
        "output_df = output_df.drop_duplicates()\n",
        "after = len(output_df)\n",
        "if before > after:\n",
        "    print(f\"åˆ é™¤é‡å¤è¡Œ: -{before - after} è¡Œ\")\n",
        "\n",
        "# æ’åº\n",
        "output_df = output_df.sort_values(['Stkcd', 'Accper', 'Typrep']).reset_index(drop=True)\n",
        "\n",
        "# ç»Ÿè®¡ä¿¡æ¯\n",
        "print(f\"\\næœ€ç»ˆæ•°æ®å½¢çŠ¶: {output_df.shape}\")\n",
        "print(f\"æ ·æœ¬é‡: {len(output_df)}\")\n",
        "print(f\"å…¬å¸æ•°: {output_df['Stkcd'].nunique()}\")\n",
        "print(f\"å¹´ä»½èŒƒå›´: {output_df['Accper'].min()}-{output_df['Accper'].max()}\")\n",
        "print(f\"æŠ¥è¡¨ç±»å‹: {output_df['Typrep'].value_counts().to_dict()}\")\n",
        "print(f\"è¿è§„æ¯”ä¾‹: {output_df['isviolation'].mean():.2%}\")\n",
        "print(f\"STæ¯”ä¾‹: {output_df['isST'].mean():.2%}\")\n",
        "\n",
        "# æ˜¾ç¤ºå‰å‡ è¡Œ\n",
        "print(\"\\nå‰5è¡Œé¢„è§ˆ:\")\n",
        "print(output_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "doux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 9: è´¨é‡æ ¡éªŒ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nâœ“ å®Œæ•´æ€§æ£€æŸ¥:\")\n",
        "key_completeness = ((output_df['Stkcd'].notna()) & (output_df['Accper'].notna())).mean()\n",
        "print(f\"  ä¸»é”®å®Œæ•´ç‡: {key_completeness:.2%}\")\n",
        "\n",
        "print(\"\\nâœ“ ä¸€è‡´æ€§æ£€æŸ¥:\")\n",
        "print(f\"  æ•°æ®é‡: {len(output_df)} æ¡\")\n",
        "print(f\"  å…¬å¸æ•°: {output_df['Stkcd'].nunique()} å®¶\")\n",
        "print(f\"  å¹´ä»½: {output_df['Accper'].min()}-{output_df['Accper'].max()}\")\n",
        "\n",
        "print(\"\\nâœ“ åˆç†æ€§æ£€æŸ¥:\")\n",
        "violation_rate = output_df['isviolation'].mean()\n",
        "print(f\"  è¿è§„æ¯”ä¾‹: {violation_rate:.2%} (åˆç†èŒƒå›´: 3-10%)\")\n",
        "st_rate = output_df['isST'].mean()\n",
        "print(f\"  STæ¯”ä¾‹: {st_rate:.2%}\")\n",
        "\n",
        "print(\"\\nâœ“ æŠ½æ ·éªŒè¯:\")\n",
        "sample = output_df.head(3)\n",
        "for idx, row in sample.iterrows():\n",
        "    print(f\"  Stkcd={int(row['Stkcd'])}, Year={int(row['Accper'])}, Typrep={row['Typrep']}, \"\n",
        "          f\"Indcd={row['Indcd']}, isviolation={row['isviolation']}, isST={row['isST']}\")\n",
        "\n",
        "print(\"\\nâœ“ è´¨é‡æ ¡éªŒé€šè¿‡\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "xie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Step 10: å¯¼å‡ºæ–‡ä»¶\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, f'{GROUP_ID}-preprocessed.csv')\n",
        "\n",
        "try:\n",
        "    output_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
        "    print(f\"\\nâœ“ æˆåŠŸä¿å­˜: {output_file}\")\n",
        "    print(f\"âœ“ æ–‡ä»¶å¤§å°: {size_mb:.2f} MB\")\n",
        "    print(f\"âœ“ æ•°æ®å½¢çŠ¶: {output_df.shape}\")\n",
        "    \n",
        "    print(f\"\\nå‰5è¡Œé¢„è§ˆ:\")\n",
        "    print(output_df.head())\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"æ•°æ®é¢„å¤„ç†å®Œæˆï¼\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"å®Œæˆæ—¶é—´: {end_time}\")\n",
        "    print(f\"è¾“å‡ºæ–‡ä»¶: {output_file}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ä¿å­˜å¤±è´¥: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦\n",
        "\n",
        "æŸ¥çœ‹æœ€ç»ˆæ•°æ®çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®ç»Ÿè®¡æ‘˜è¦\n",
        "print(\"=\" * 80)\n",
        "print(\"æ•°æ®ç»Ÿè®¡æ‘˜è¦\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n1. åŸºæœ¬ç»Ÿè®¡:\")\n",
        "print(f\"   æ€»æ ·æœ¬æ•°: {len(output_df):,}\")\n",
        "print(f\"   å…¬å¸æ•°: {output_df['Stkcd'].nunique():,}\")\n",
        "print(f\"   å¹´ä»½èŒƒå›´: {int(output_df['Accper'].min())}-{int(output_df['Accper'].max())}\")\n",
        "print(f\"   ç‰¹å¾æ•°: {len(output_df.columns)}\")\n",
        "\n",
        "print(f\"\\n2. æ ‡ç­¾åˆ†å¸ƒ:\")\n",
        "print(f\"   è¿è§„æ ·æœ¬: {output_df['isviolation'].sum():,} ({output_df['isviolation'].mean():.2%})\")\n",
        "print(f\"   STæ ·æœ¬: {output_df['isST'].sum():,} ({output_df['isST'].mean():.2%})\")\n",
        "print(f\"   STä¸”è¿è§„: {((output_df['isST']==1) & (output_df['isviolation']==1)).sum():,} \"\n",
        "      f\"({((output_df['isST']==1) & (output_df['isviolation']==1)).mean():.2%})\")\n",
        "\n",
        "print(f\"\\n3. æŠ¥è¡¨ç±»å‹åˆ†å¸ƒ:\")\n",
        "print(output_df['Typrep'].value_counts())\n",
        "\n",
        "print(f\"\\n4. è¡Œä¸šä»£ç ç»Ÿè®¡:\")\n",
        "if output_df['Indcd'].notna().sum() > 0:\n",
        "    print(f\"   éç©ºè¡Œä¸šä»£ç : {output_df['Indcd'].notna().sum():,} ({output_df['Indcd'].notna().mean():.2%})\")\n",
        "    print(f\"   å”¯ä¸€è¡Œä¸šæ•°: {output_df['Indcd'].nunique()}\")\n",
        "    print(f\"   å‰10ä¸ªè¡Œä¸š:\")\n",
        "    print(output_df['Indcd'].value_counts().head(10))\n",
        "else:\n",
        "    print(\"   æ‰€æœ‰è¡Œä¸šä»£ç ä¸ºç©º\")\n",
        "\n",
        "print(f\"\\n5. ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆå‰10ä¸ªç¼ºå¤±ç‡æœ€é«˜çš„åˆ—ï¼‰:\")\n",
        "missing_stats = (output_df.isnull().sum() / len(output_df) * 100).sort_values(ascending=False)\n",
        "print(missing_stats.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
